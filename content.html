<h1>How Computers Use Numbers</h1>

<p>
    We have tricked electrons into doing simple logic, but that's not too useful on its own.
    While this is enough to perform basically any computation, <em>actually having numbers</em> would be quite useful.
    So let's invent them!
</p>

<h2>The goal</h2>
<p>
    First, we need to find what traits a good number format has, and thus we should be aiming for.
    Here are the ones modern computers optimize for:
</p>
<dl>
    <dt>Usefulness in real programs</dt>
    <dd>
        Perhaps the most important trait, as the whole point of having the capability to do math is for programs to use it.
        Additionally, fitting as many use cases into as few formats as possible would be ideal,
        as having more ways to store numbers than necessary would make processors a lot more expensive and power-hungry.
    </dd>
    <dt>Simplicity</dt>
    <dd>
        Similarly to avoiding too many formats, straightforward logic would also save on price and power usage.
    </dd>
    <dt>Efficient memory usage</dt>
    <dd>
        Memory is a limited resource; therefore using it as efficiently as possible would be ideal,
        for example by avoiding duplicate values.
    </dd>
</dl>

<h2>Memory</h2>
<memory-demo></memory-demo>
<p>
    Memory is a collection of cells holding one of two values (i.e. a bit). These can represent basically anything that has two states,
    though usually they are visualized using ones and zeroes.
</p>
<memory-demo-controller></memory-demo-controller>
<p>
    Note that bits are usually grouped in eights, so using a multiple of 8 would be easiest to work with.
    We'll use just 8 bits for simplicity, but all of the following number formats come in a variety of sizes.
</p>

<h2>Counting with bits</h2>
<p>
    Similarly to how we use the 10 digits to count in base 10, we can use bits to count in base 2.
    By assigning each bit to a power of 2, we can use our 8 bits to store numbers between 0 and 255
    (<math display="inline">
        <mrow>
            <msup>
                <mn>2</mn>
                <mn>8</mn>
            </msup>
            <mo>-</mo>
            <mn>1</mn>
        </mrow>
    </math>).
</p>
<integer-demo type="unsigned" value="53"></integer-demo>

<h2>Negative numbers</h2>
<p>
    An obvious way of defining negative numbers is to just use 1 bit as a sign.
</p>
<integer-demo type="sign-bit" value="0x92"></integer-demo>
<p>
    However, this way of doing it has one tiny issue: there are two ways to write 0.
</p>
<integer-demo type="sign-bit" value="0" locked></integer-demo>
<integer-demo type="sign-bit" value="0x80" locked></integer-demo>
<p>
    Instead, we could interpret the number as if it was unsigned, but then shift it down by 128
    (<math display="inline">
        <mrow>
            <msup>
                <mn>2</mn>
                <mn>7</mn>
            </msup>
        </mrow>
    </math>)
    to be able to represent numbers from -128 to 127:
</p>
<integer-demo type="shifted" value="0x80"></integer-demo>
<p>
    At first glance this way of doing negatives seems perfect!
    After all it's very simple, it covers a useful range and all the bit patterns are unique.
</p>
<p>
    But we can do better.
</p>


<hr>

<h2>A binary point in the middle</h2>
<p>
    <ul>
        <li>fixed point demonstration</li>
        <li>why it's not that great</li>
    </ul>
</p>

<h2>But we can do better</h2>
<p>
    <ul>
        <li>we want to make the best of our 2^32 values</li>
        <li>high precision for small numbers and low precision for large numbers</li>
        <li>we can't represent all the real numbers</li>
        <li>the obvious solution is rounding
            <ul>
                <li>this makes it so each value actually represents a range</li>
            </ul>
        </li>
    </ul>
</p>

<h2>Moving the binary point</h2>
<p>
    <ul>
        <li>dedicate a few bits to point position</li>
        <li>problem: multiple bit patterns for the same values</li>
    </ul>
</p>

<h2>Scientific notation</h2>
<p>
    <ul>
        <li>how it avoids the problem</li>
        <li>let's do it! instead of moving the binary point, treat some bits as an exponent</li>
    </ul>
</p>

<h3>Removing the invalid cases</h3>
<p>
    <ul>
        <li>for scientific notation, the first digit must be non-zero for unique representations</li>
        <li>so let's just put a predefined 1 there</li>
        <li>problem: can't write 0 with the trick</li>
        <li>easy solution: set the all zeroes bit pattern to be equal to 0</li>
    </ul>
</p>

<h2>Negative numbers again</h2>
<p>
    <ul>
        <li>we could try doing shifting or something like two's complement again, but that'd be really complex and hard to work with</li>
        <li>instead just make 1 bit be the sign</li>
        <li>note that +/- 0 exists now, but that's not a big deal (they're just the two different ends of the range)</li>
    </ul>
</p>

<h2>Fully covering the number line</h2>
<p>
    <ul>
        <li>remember that the value 0 actually represents numbers close to zero
            <ul>
                <li>this means that 1/0 has a value, but we can't assign it a value because we don't have enough information</li>
                <li>so, let's make a special value, representing a very large number</li>
                <li>what properties should it have</li>
            </ul>
        </li>
    </ul>
</p>

<h3>What about 0/0?</h3>
<p>
    <ul>
        <li>a small number we don't know exactly divided by another small number we don't know exactly could be literally anything</li>
        <li>also, representing various kinds of errors (such as square roots of negative numbers) could be useful</li>
        <li>let's dedicate the top exponent to these error (NaN) values</li>
        <li>we can also put the infinities in there</li>
    </ul>
</p>

<h2>Squeezing out a bit more range</h3>
<p>
    <ul>
        <li>instead of dedicating a value to 0, let's set the lowest exponent's leading digit to 0 but increase it by 1</li>
        <li>this keeps the ability to write 0 and uniqueness, but now we can write much smaller numbers</li>
        <li>(how small exactly?)</li>
    </ul>
</p>

<hr>

<h2>Conclusion</h2>
<p>
    <ul>
        <li>those are complete descriptions of how most modern computers store integers and floats</li>
        <li>note that these formats come in a variety of sizes</li>
        <li>these are also used as building blocks to make other funky formats
            <ul>
                <li>complex numbers (pair of floats)</li>
                <li>rationals (numerator + denominator)</li>
                <li>arbitrary-precision integers (size + data)</li>
                <li>decimal numbers (digits 0-9 encoded as bits)</li>
                <li>etc.</li>
            </ul>
        </li>
    </ul>
</p>
