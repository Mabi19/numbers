<h1>How Computers Use Numbers</h1>

<p>
    We have tricked electrons into doing simple logic, but that's not too useful on its own.
    While this is enough to perform basically any computation, <em>actually having numbers</em> would be quite useful.
    So let's invent them!
</p>

<h2>The goal</h2>
<p>
    First, we need to find what traits a good number format has, and thus we should be aiming for.
    Here are the ones modern computers optimize for:
</p>
<dl>
    <dt>Usefulness in real programs</dt>
    <dd>
        Perhaps the most important trait, as the whole point of having the capability to do math is for programs to use it.
        Additionally, fitting as many use cases into as few formats as possible would be ideal,
        as having more ways to store numbers than necessary would make processors a lot more expensive and power-hungry.
    </dd>
    <dt>Simplicity</dt>
    <dd>
        Similarly to avoiding too many formats, straightforward logic would also save on price and power usage.
    </dd>
    <dt>Efficient memory usage</dt>
    <dd>
        Memory is a limited resource; therefore using it as efficiently as possible would be ideal,
        for example by avoiding duplicate values.
    </dd>
</dl>

<h2>Memory</h2>
<memory-demo></memory-demo>
<p>
    Memory is a collection of cells holding one of two values (i.e. a bit). These can represent basically anything that has two states,
    though usually they are visualized using ones and zeroes.
</p>
<memory-demo-controller></memory-demo-controller>
<p>
    Note that bits are usually grouped in eights, so using a multiple of 8 would be easiest to work with.
    We'll use just 8 bits for simplicity, but all of the following number formats come in a variety of sizes.
</p>

<h2>Counting with bits</h2>
<p>
    Similarly to how we use the 10 digits to count in base 10, we can use bits to count in base 2.
    By assigning each bit to a power of 2, we can use our 8 bits to store numbers between 0 and 255
    (<math display="inline">
        <mrow>
            <msup>
                <mn>2</mn>
                <mn>8</mn>
            </msup>
            <mo>-</mo>
            <mn>1</mn>
        </mrow>
    </math>).
    This format is called an <em>8-bit unsigned integer</em>.
</p>
<integer-demo type="unsigned" value="53"></integer-demo>

<h2>Negative numbers</h2>
<p>
    An obvious way of defining negative numbers (<em>signed integers</em>) is to just use 1 bit as a sign.
</p>
<integer-demo type="sign-bit" value="0x92"></integer-demo>
<p>
    However, this way of doing it has one tiny issue: there are two ways to write 0.
</p>
<integer-demo type="sign-bit" value="0" locked></integer-demo>
<integer-demo type="sign-bit" value="0x80" locked></integer-demo>
<p>
    Instead, we could interpret the number as if it was unsigned, but then offset it down by 128
    (<math display="inline">
        <mrow>
            <msup>
                <mn>2</mn>
                <mn>7</mn>
            </msup>
        </mrow>
    </math>)
    to be able to represent numbers from -128 to 127:
</p>
<integer-demo type="shifted" value="0x80"></integer-demo>
<p>
    At first glance this way of doing negatives seems perfect!
    After all it's very simple, it covers a useful range and all the bit patterns correspond to unique integers.
</p>
<p>
    But we can do better.
</p>

<h3>Two's complement</h3>
<p>
    While implementing negative numbers via a fixed offset seems relatively optimal,
    we can actually significantly cut down on the amount of circuitry we need via a neat trick!
</p>
<p>
    A hypothetical computer which uses offset signed integers and regular unsigned integers would need to implement
    arithmetic and comparisons for both signed and unsigned integers, as well as conversions between the two.
</p>
<p>
    Since arithmetic and conversions are only defined for where the ranges of the two formats overlap,
    could we have the 0-127 range use the same bit patterns for both signed and unsigned integers?
    It turns out that the answer is yes! There are several ways to achieve this,
    but by far the most common approach is making the most significant bit subtract 128 from the total instead of adding it.
</p>
<integer-demo type="twos-complement" value="0xfd" id="twoc-switching-demo"></integer-demo>
<p>
    Remember that "unsigned integer" and "two's complement" are just ways of interpreting a collection of bits.
    Because all the relevant bit patterns match, we can convert for free by just changing how we're interpreting the data.
</p>
<type-switcher
    target="#twoc-switching-demo"
    types='[{ "id": "unsigned", "name": "Unsigned" }, { "id": "twos-complement", "name": "Two&apos;s complement" }]'
    selected="twos-complement">
</type-switcher>
<p>
    (note: this toggle affects the above demo)
</p>
<p>
    Additionally, two's complement allows us to drop the circuitry for signed addition,
    subtraction and multiplication, and if you're familiar with the
    <a href="https://youtu.be/3gyHKCDq1YA">p-adic numbers</a> you might already have noticed why.
</p>
<div>
    TODO: signed addition demo
</div>

<hr>

<h2>A binary point in the middle</h2>
<p>
    Since we have mastered integers, we can now go fractional.
    Let's start by taking a regular 32-bit unsigned integer and putting a binary point in the middle:
</p>
<fixed-point-demo value="0x00038000"></fixed-point-demo>
<p>
    This is called fixed point, and it's <em>not terrible</em>.
    We have drastically cut the range of numbers we can represent even without including negative numbers
    (as an integer, we could represent the range 0-4,294,967,295 with 32 bits, and this format has only 0 to just under 65536).
    We could try putting the binary point somewhere else, but there really isn't a spot that gives both decent range and precision.
</p>
<p>
    However, it is very simple, and the fact that addition and subtraction work the same as for integers
    does make this a good option in some niche cases.
</p>

<h2>But we can do better</h2>
<p>
    Having uniformly spaced values, despite being great for integers, hasn't worked out so well for fractions.
    So, how should we distribute them? Well, since we want to fit as many use cases as possible into one format,
    both atom-sized and universe-sized numbers should be supported.
    But in practice, universe-sized numbers are less precise (in absolute terms) than atom-sized ones.
</p>

<h2>Moving the binary point</h2>
<p>
    Since larger numbers are inherently less precise, could we skip defining all those decimals for large values?
    Well, yes! Let's dedicate, say, 5 bits to defining where the binary point is.
    For large values, we can move the point after most of the bits, and for small numbers, we can put it
    after one zero bit.
</p>
<!-- TODO: find a better example value -->
<moving-point-demo value="0x30005000"></moving-point-demo>
<p>
    Also, since 5 bits give us more possible bit positions than there actually are with the remaining 27 value bits,
    we can use the remaining ones for <em>hypervalues</em>, by putting extra <em>virtual zeroes</em> to fill in the space.
</p>
<moving-point-demo value="0xe8000003" hypervalues></moving-point-demo>
<p>
    However, even with this range extension, the format is still extremely wasteful - for example, you can write 0
    in 32 different ways as the point position does not matter.
    <!--
        Which representations are canonical:
        fractional values (27 point patterns): half of them, since ending with a 0 means we can shift the number to the right
        and add 1 to the point position
        integers (5 point patterns): half of them for the first 4 patterns; if it ends with a 0, we can shift the number to the right
        and subtract 1 from the point position
        11111 point pattern is all canonical
    -->
    In fact, only <strong>~52%</strong> of all bit patterns produce unique values in this system. Yikes!
</p>

<h2>Scientific notation</h2>
<p>
    In order to ensure uniqueness, we can take inspiration from a way you might see numbers written down in the real world -
    scientific notation! We will need to adapt it to better suit our needs, though.
</p>
<p>
    Remember that a number in scientific notation looks like
    <math display="inline">
        <mrow>
            <mn>2.574</mn>
            <mo>&sdot;</mo>
            <msup>
                <mn>10</mn>
                <mn>4</mn>
            </msup>
        </mrow>
    </math>,
    where the mantissa is a number with one digit in front of the decimal point and an arbitrary amount of digits after,
    and the exponent is any integer.
</p>
<p>
    For the exponent, let's start by dedicating 8 of our 32 bits to it.
    Since we need both positive and negative exponents to represent large and small numbers respectively,
    we need to use a signed format.
    <!-- TODO: illustrate this -->
    We could try doing two's complement again, but we wouldn't actually be able to take advantage of its benefits very well in this case.
    Instead, let's just make it an unsigned integer and subtract 127 from it, as that would make the underlying implementation simpler.
</p>
<p>
    For the mantissa, let's just use the remaining 24 bits as a fixed-point number with 1 binary digit before the binary point.
    As for its sign, note that two's complement or shifting would be incredibly awkward to use here.
    This is because we need to represent a balanced range,
    and ideally there would be an equal amount of values on either side of 0.
    But that would make the total amount of values odd (<math display="inline">
        <mrow>
            <mn>2</mn>
            <mo>&sdot;</mo>
            <mi>positive values</mi>
            <mo>+</mo>
            <mn>1</mn>
        </mrow>
    </math>), and the amount of values we have to work with is even (<math display="inline">
        <mrow>
            <msup>
                <mn>2</mn>
                <mi>bits</mi>
            </msup>
        </mrow>
    </math>).
    Therefore, since we can't use the extra value effectively, let's just waste it in the simplest way possible:
    dedicate one bit of the mantissa to the sign, negating the value when the bit is 1. (The extra value goes to -0.)
</p>
<floating-point-demo type="naive"></floating-point-demo>
<h3>Actually ensuring uniqueness</h3>
<p>
    Our format does not guarantee uniqueness yet. This is because we skipped one requirement of scientific notation:
    the first digit of the mantissa must be non-zero (otherwise you can change the exponent and get the same value).
    So, we have to make the first digit of the mantissa non-zero.
</p>
<p>
    Notice that since we're working in binary, there is only one non-zero digit: one!
    That means that we can omit storing it entirely, which allows us to double the mantissa's precision. Nice!
</p>
<h3>Oh wait</h3>
<p>
    Remember when I said that the first digit of the mantissa must be non-zero? That's true, except for exactly one number: 0.
    Thus, we can't write it anymore. So, let's just set the all zeroes (except for possibly the sign bit)
    bit pattern to represent &pm;0.
</p>
<!-- TODO: exponent table? -->

<h2>Handling math with varying precision</h2>
<p>
    Because of the nature of having limited precision, we're going to encounter results that don't have bit representations
    quite often when doing basically any kind of math. (Indeed, the 4 billion values cover exactly 0% of the number line.)
</p>
<p>
    The obvious solution to that is rounding: after each mathematical operation, if the result does not have a bit pattern in our system,
    it snaps to the closest available value.
</p>
<p>
    Notice that now, each bit pattern actually represents a range of values -
    the actual result that rounded to it could have been anywhere within the rounding window.
</p>
<p>
    Also, remember how we had an extra zero? We can now have +0 represent the positive half of the range and vice versa.
</p>

<h2>A giant gap</h2>
<p>
    Our format is currently <em>not great</em> at representing small numbers.
    Sure, we can store 0, but then we have a comparatively huge gap from anything else:
    the smallest positive numbers in our format are 2^-127 + 2^-150, then 2^-127 + 2^-149.
    That means that the second gap is only 0.000012% of the first one.
</p>
<p>
    To rectify our small number issues, we can make the whole lowest exponent have its mantissa start with a 0
    instead of just one specific value.
    (We also need to increase it by 1 to not leave a hole where the 2^-127-sized numbers originally were).
    Let's call these <em>subnormal numbers</em>.
</p>
<p>
    The smallest representable positive numbers are now 2^-149, then 2 * 2^-149, 3 * 2^-149 and so on.
    That means that not only do we have much more uniform gaps, but we even extended the range a little
    (though at the cost of some precision).
</p>
<h2>Extending the range further</h2>
<p>
    Having a special value for numbers outside the normal range would be useful.
    For example, since zero could actually be any number close to 0, dividing by it
    <em>would result in an actual value</em>! We can't know it exactly (since we threw away that precision),
    so it would just result in the too-big sentinel.
</p>
<p>
    So, let's dedicate the largest regular value to the too-big sentinel,
    which effectively covers the part of the number line from the (new) largest regular value all the way to infinity.
    Since our format is symmetric we'll also define -Infinity to have the same bit pattern as the positive one
    except with the sign bit being 1.
</p>
<h3>Properties of an indeterminate value</h3>
<p>
    Since the infinities cover infinitely large parts of the number line, the numbers they represent could be pretty much anywhere.
    So, how does math work when &pm;&infin; is involved? Well, in practice, it only really makes sense to give
    it the same properties that &infin; (or specifically <math display="inline">
        <mrow>
            <munder>
                <mo>lim</mo>
                <mrow>
                    <mi>x</mi>
                    <mo>&rightarrow;</mo>
                    <mi>&infin;</mi>
                </mrow>
            </munder>
            <mi>x</mi>
        </mrow>
    </math>) has, namely that it's contagious - most operations involving infinities will result in another infinity:
</p>
<ul>
    <li>
        <math display="inline">
            <mrow>
                <mi>&infin;</mi><mo>+</mo><mn>1</mn><mo>=</mo><mi>&infin;</mi>
            </mrow>
        </math>
    </li>
    <li>
        <math display="inline">
            <mrow>
                <mi>&infin;</mi><mo>&sdot;</mo><mo>(</mo><mn>-2</mn><mo>)</mo><mo>=</mo><mi>-&infin;</mi>
            </mrow>
        </math>
    </li>
    <li>
        <math display="inline">
            <mrow>
                <mi>&infin;</mi><mo>+</mo><mi>&infin;</mi><mo>=</mo><mi>&infin;</mi>
            </mrow>
        </math>
    </li>
    <li>
        <math display="inline">
            <mrow>
                <mi>&infin;</mi><mo>></mo><mn>2</mn><mo>=</mo><mi>true</mi>
            </mrow>
        </math>
    </li>
    <li>
        <math display="inline">
            <mrow>
                <mi>&infin;</mi><mo>-</mo><mi>&infin;</mi><mo>=</mo><mi>?</mi>
            </mrow>
        </math>
    </li>
    <li>
        <math display="inline">
            <mrow>
                <mi>&infin;</mi><mo>&sdot;</mo><mn>0</mn><mo>=</mo><mi>?</mi>
            </mrow>
        </math>
    </li>
</ul>

<h2>Dealing with those question marks</h2>
<p>
    <ul>
        <li>a small number we don't know exactly divided by another small number we don't know exactly could be literally anything</li>
        <li>also, representing various kinds of errors (such as square roots of negative numbers) could be useful</li>
        <li>let's dedicate the top exponent to these error (NaN) values, like we did for the subnormals</li>
        <li>even weirder properties than the infinities, since this time it could truly be anything</li>
        <li>we can also put the infinities in there</li>
    </ul>
</p>

<hr>

<h2>Conclusion</h2>
<p>
    <ul>
        <li>those are complete descriptions of how most modern computers store integers and floats</li>
        <li>note that these formats come in a variety of sizes</li>
        <li>these are also used as building blocks to make other funky formats
            <ul>
                <li>complex numbers (pair of floats)</li>
                <li>rationals (numerator + denominator)</li>
                <li>arbitrary-precision integers (size + data)</li>
                <li>decimal numbers (digits 0-9 encoded as bits)</li>
                <li>etc.</li>
            </ul>
        </li>
    </ul>
</p>
