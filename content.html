<h1>How Computers Use Numbers</h1>


<p>
    We have tricked electrons into doing simple logic, but that's not too useful on its own.
    While this is enough to perform basically any computation, <em>actually having numbers</em> would be quite useful.
    So let's invent them!
</p>


<h2>Memory</h2>
<p>
    Just having the ability to do arithmetic without being able to store its results wouldn't be very useful.
    Thankfully, we have memory.
</p>

<memory-demo></memory-demo>

<p>
    Memory is a collection of cells holding one of two values (i.e. a bit). These can represent basically anything that has two states,
    though usually they are represented by ones and zeroes.
</p>

<memory-demo-controller></memory-demo-controller>

<p>
    Note that bits are usually grouped in eights, so using a multiple of 8 would be easiest to work with.
    We'll be working with 32 bits here, but all of the following number formats come in a variety of sizes.
</p>


<h2>Counting with bits</h2>
<p>
    Similarly to how we use the 10 digits to count in base 10, we can use bits to count in base 2.
    By assigning each bit to a power of 2, we can use 32 bits to store numbers between 0 and 4,294,967,295
    (<math display="inline">
        <mrow>
            <msup>
                <mn>2</mn>
                <mn>32</mn>
            </msup>
            <mo>-</mo>
            <mn>1</mn>
        </mrow>
    </math>).
</p>

<integer-demo type="unsigned" value="53"></integer-demo>

<h2>Negative numbers</h2>
<p>
    <ul>
        <li>two's complement</li>
        <li>parallels to the p-adics</li>
    </ul>
</p>

<hr>

<h2>A binary point in the middle</h2>
<p>
    <ul>
        <li>fixed point demonstration</li>
        <li>why it's not that great</li>
    </ul>
</p>

<h2>But we can do better</h2>
<p>
    <ul>
        <li>we want to make the best of our 2^32 values</li>
        <li>high precision for small numbers and low precision for large numbers</li>
        <li>we can't represent all the real numbers</li>
        <li>the obvious solution is rounding
            <ul>
                <li>this makes it so each value actually represents a range</li>
            </ul>
        </li>
    </ul>
</p>

<h2>Moving the binary point</h2>
<p>
    <ul>
        <li>dedicate a few bits to point position</li>
        <li>problem: multiple bit patterns for the same values</li>
    </ul>
</p>

<h2>Scientific notation</h2>
<p>
    <ul>
        <li>how it avoids the problem</li>
        <li>let's do it! instead of moving the binary point, treat some bits as an exponent</li>
    </ul>
</p>

<h3>Removing the invalid cases</h3>
<p>
    <ul>
        <li>for scientific notation, the first digit must be non-zero for unique representations</li>
        <li>so let's just put a predefined 1 there</li>
        <li>problem: can't write 0 with the trick</li>
        <li>easy solution: set the all zeroes bit pattern to be equal to 0</li>
    </ul>
</p>

<h2>Negative numbers again</h2>
<p>
    <ul>
        <li>we could try doing shifting or something like two's complement again, but that'd be really complex and hard to work with</li>
        <li>instead just make 1 bit be the sign</li>
        <li>note that +/- 0 exists now, but that's not a big deal (they're just the two different ends of the range)</li>
    </ul>
</p>

<h2>Fully covering the number line</h2>
<p>
    <ul>
        <li>remember that the value 0 actually represents numbers close to zero
            <ul>
                <li>this means that 1/0 has a value, but we can't assign it a value because we don't have enough information</li>
                <li>so, let's make a special value, representing a very large number</li>
                <li>what properties should it have</li>
            </ul>
        </li>
    </ul>
</p>

<h3>What about 0/0?</h3>
<p>
    <ul>
        <li>a small number we don't know exactly divided by another small number we don't know exactly could be literally anything</li>
        <li>also, representing various kinds of errors (such as square roots of negative numbers) could be useful</li>
        <li>let's dedicate the top exponent to these error (NaN) values</li>
        <li>we can also put the infinities in there</li>
    </ul>
</p>

<h2>Squeezing out a bit more range</h3>
<p>
    <ul>
        <li>instead of dedicating a value to 0, let's set the lowest exponent's leading digit to 0 but increase it by 1</li>
        <li>this keeps the ability to write 0 and uniqueness, but now we can write much smaller numbers</li>
        <li>(how small exactly?)</li>
    </ul>
</p>

<hr>

<h2>Conclusion</h2>
<p>
    <ul>
        <li>those are complete descriptions of how most modern computers store integers and floats</li>
        <li>note that these formats come in a variety of sizes</li>
        <li>these are also used as building blocks to make other funky formats
            <ul>
                <li>complex numbers (pair of floats)</li>
                <li>rationals (numerator + denominator)</li>
                <li>arbitrary-precision integers (size + data)</li>
                <li>decimal numbers (digits 0-9 encoded as bits)</li>
                <li>etc.</li>
            </ul>
        </li>
    </ul>
</p>
